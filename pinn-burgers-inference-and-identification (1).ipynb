{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow==1.15.2\n!git clone https://github.com/maziarraissi/PINNs.git\n!pip install pyDOE==0.3.7","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-21T07:27:57.511621Z","iopub.execute_input":"2021-11-21T07:27:57.51209Z","iopub.status.idle":"2021-11-21T07:29:23.120093Z","shell.execute_reply.started":"2021-11-21T07:27:57.512002Z","shell.execute_reply":"2021-11-21T07:29:23.119095Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib as mpl\n#mpl.use('pgf')\nimport matplotlib.pyplot as plt\n\ndef figsize(scale, nplots = 1):\n    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n    fig_height = nplots*fig_width*golden_mean              # height in inches\n    fig_size = [fig_width,fig_height]\n    return fig_size\n\n# I make my own newfig and savefig functions\ndef newfig(width, nplots = 1):\n    fig = plt.figure(figsize=figsize(width, nplots))\n    ax = fig.add_subplot(111)\n    return fig, ax\n\ndef savefig(filename, crop = True):\n    if crop == True:\n#        plt.savefig('{}.pgf'.format(filename), bbox_inches='tight', pad_inches=0)\n        plt.savefig('{}.pdf'.format(filename), bbox_inches='tight', pad_inches=0)\n        plt.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\n    else:\n#        plt.savefig('{}.pgf'.format(filename))\n        plt.savefig('{}.pdf'.format(filename))\n        plt.savefig('{}.eps'.format(filename))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T07:29:23.122067Z","iopub.execute_input":"2021-11-21T07:29:23.122337Z","iopub.status.idle":"2021-11-21T07:29:23.132189Z","shell.execute_reply.started":"2021-11-21T07:29:23.122303Z","shell.execute_reply":"2021-11-21T07:29:23.13131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io\nfrom scipy.interpolate import griddata\nfrom pyDOE import lhs\nfrom mpl_toolkits.mplot3d import Axes3D\nimport time\nimport matplotlib.gridspec as gridspec\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nnp.random.seed(1234)\ntf.set_random_seed(1234)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T07:29:23.133761Z","iopub.execute_input":"2021-11-21T07:29:23.134193Z","iopub.status.idle":"2021-11-21T07:29:26.185625Z","shell.execute_reply.started":"2021-11-21T07:29:23.134149Z","shell.execute_reply":"2021-11-21T07:29:26.184776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PhysicsInformedNN:\n    # Initialize the class\n    def __init__(self, X_u, u, X_f, layers, lb, ub, nu):\n        \n        self.lb = lb\n        self.ub = ub\n    \n        self.x_u = X_u[:,0:1]    # X-coordinates of boundary points\n        self.t_u = X_u[:,1:2]    # time-coordinates of boundary points\n        \n        self.x_f = X_f[:,0:1]    # X-coordinates of Internal+Boundary points\n        self.t_f = X_f[:,1:2]    # time-coordinates of Interval+Boundary points\n        \n        self.u = u\n        \n        self.layers = layers\n        self.nu = nu              # Noise\n        \n        # Initialize NNs\n        self.weights, self.biases = self.initialize_NN(layers) \n        \n        # tf placeholders and graph\n        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                                     log_device_placement=True)) # Define session\n        \n        # Placeholders for Initial Conditions+Boundary\n        self.x_u_tf = tf.placeholder(tf.float32, shape=[None, self.x_u.shape[1]]) # Nonex1\n        self.t_u_tf = tf.placeholder(tf.float32, shape=[None, self.t_u.shape[1]]) # Nonex1\n        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]]) # Nonex1\n        \n        # Placeholders for everything\n        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]]) # Nonex1\n        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]]) # Nonex1\n                \n        self.u_pred = self.net_u(self.x_u_tf, self.t_u_tf)  # Boundary+Initial points: Supervised\n        self.f_pred = self.net_f(self.x_f_tf, self.t_f_tf)  # All points\n        \n        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\    # Minimize over initial conditions\n                    tf.reduce_mean(tf.square(self.f_pred))            # Minimize over all points\n                              \n        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n                                                                method = 'L-BFGS-B', \n                                                                options = {'maxiter': 50000,\n                                                                           'maxfun': 50000,\n                                                                           'maxcor': 50,\n                                                                           'maxls': 50,\n                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n        \n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n                \n    def initialize_NN(self, layers):       \n        weights = []\n        biases = []\n        num_layers = len(layers) \n        for l in range(0,num_layers-1):\n            W = self.xavier_init(size=[layers[l], layers[l+1]])\n            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n            weights.append(W)\n            biases.append(b)        \n        return weights, biases\n        \n    def xavier_init(self, size):    ## Xavier Init\n        in_dim = size[0]\n        out_dim = size[1]        \n        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n    \n    def neural_net(self, X, weights, biases):   ## Neural Net: Activation-Tanh+ last layer Linear \n        num_layers = len(weights) + 1\n        \n        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n        for l in range(0,num_layers-2):\n            W = weights[l]\n            b = biases[l]\n            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n        W = weights[-1]\n        b = biases[-1]\n        Y = tf.add(tf.matmul(H, W), b)\n        return Y\n            \n    def net_u(self, x, t):\n        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)  # x,t as input; u as output\n        return u\n    \n    def net_f(self, x,t):      ## Differential Equation\n        u = self.net_u(x,t)\n        u_t = tf.gradients(u, t)[0]\n        u_x = tf.gradients(u, x)[0]\n        u_xx = tf.gradients(u_x, x)[0]\n        f = u_t + u*u_x - self.nu*u_xx\n        \n        return f\n    \n    def callback(self, loss):\n        print('Loss:', loss)\n        \n    def train(self):  ## Minimize Loss\n        \n        tf_dict = {self.x_u_tf: self.x_u, self.t_u_tf: self.t_u, self.u_tf: self.u,\n                   self.x_f_tf: self.x_f, self.t_f_tf: self.t_f}\n                                                                                                                          \n        self.optimizer.minimize(self.sess, \n                                feed_dict = tf_dict,         \n                                fetches = [self.loss], \n                                loss_callback = self.callback)        \n                                    \n    \n    def predict(self, X_star): ## Predict \n                \n        u_star = self.sess.run(self.u_pred, {self.x_u_tf: X_star[:,0:1], self.t_u_tf: X_star[:,1:2]})  \n        f_star = self.sess.run(self.f_pred, {self.x_f_tf: X_star[:,0:1], self.t_f_tf: X_star[:,1:2]})\n               \n        return u_star, f_star","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:17:01.76802Z","iopub.execute_input":"2021-11-21T04:17:01.768304Z","iopub.status.idle":"2021-11-21T04:17:01.79581Z","shell.execute_reply.started":"2021-11-21T04:17:01.768274Z","shell.execute_reply":"2021-11-21T04:17:01.795053Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nu = 0.01/np.pi\nnoise = 0.0        \n\nN_u = 100\nN_f = 10000\nlayers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n\ndata = scipy.io.loadmat('./PINNs/appendix/Data/burgers_shock.mat')\n\nt = data['t'].flatten()[:,None]\nx = data['x'].flatten()[:,None]\nExact = np.real(data['usol']).T\n\nX, T = np.meshgrid(x,t)\n\nX_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\nu_star = Exact.flatten()[:,None]              \n\n# Doman bounds\nlb = X_star.min(0)\nub = X_star.max(0)    \n\n# Initial Condition\nxx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\nuu1 = Exact[0:1,:].T\n\n# Boundary Condition x=-1\nxx2 = np.hstack((X[:,0:1], T[:,0:1]))\nuu2 = Exact[:,0:1]\n\n# Boundary Condition x=1 \nxx3 = np.hstack((X[:,-1:], T[:,-1:]))\nuu3 = Exact[:,-1:]\n\nX_u_train = np.vstack([xx1, xx2, xx3])\nX_f_train = lb + (ub-lb)*lhs(2, N_f)          #  Sample from within the boundary using Latin Hypercubic Sampling\nX_f_train = np.vstack((X_f_train, X_u_train)) # Training setpoints (Within+Initial+Boundary)\nu_train = np.vstack([uu1, uu2, uu3])\n\n# Sample from the boundary and initial conditions and make a train set (Note: These values are fixed)\nidx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\nX_u_train = X_u_train[idx, :]\nu_train = u_train[idx,:]\n\nmodel = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub, nu)\n\nstart_time = time.time()                \nmodel.train()\nelapsed = time.time() - start_time                \nprint('Training time: %.4f' % (elapsed))\n\nu_pred, f_pred = model.predict(X_star)  ## Predict over all points\n\nerror_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\nprint('Error u: %e' % (error_u))                     \n\n\nU_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')  \nError = np.abs(Exact - U_pred)++","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:17:43.168734Z","iopub.execute_input":"2021-11-21T04:17:43.169399Z","iopub.status.idle":"2021-11-21T04:26:54.732532Z","shell.execute_reply.started":"2021-11-21T04:17:43.169361Z","shell.execute_reply":"2021-11-21T04:26:54.731581Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################################################################\n############################# Plotting ###############################\n######################################################################    \n\nfig, ax = newfig(1.0, 1.1)\nax.axis('off')\n\n####### Row 0: u(t,x) ##################    \ngs0 = gridspec.GridSpec(1, 2)\ngs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\nax = plt.subplot(gs0[:, :])\n\nh = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n              extent=[t.min(), t.max(), x.min(), x.max()], \n              origin='lower', aspect='auto')\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nfig.colorbar(h, cax=cax)\n\nax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n\nline = np.linspace(x.min(), x.max(), 2)[:,None]\nax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\nax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\nax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n\nax.set_xlabel('$t$')\nax.set_ylabel('$x$')\nax.legend(frameon=False, loc = 'best')\nax.set_title('$u(t,x)$', fontsize = 10)\n\n####### Row 1: u(t,x) slices ##################    \ngs1 = gridspec.GridSpec(1, 3)\ngs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n\nax = plt.subplot(gs1[0, 0])\nax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \nax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\nax.set_xlabel('$x$')\nax.set_ylabel('$u(t,x)$')    \nax.set_title('$t = 0.25$', fontsize = 10)\nax.axis('square')\nax.set_xlim([-1.1,1.1])\nax.set_ylim([-1.1,1.1])\n\nax = plt.subplot(gs1[0, 1])\nax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \nax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\nax.set_xlabel('$x$')\nax.set_ylabel('$u(t,x)$')\nax.axis('square')\nax.set_xlim([-1.1,1.1])\nax.set_ylim([-1.1,1.1])\nax.set_title('$t = 0.50$', fontsize = 10)\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n\nax = plt.subplot(gs1[0, 2])\nax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \nax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\nax.set_xlabel('$x$')\nax.set_ylabel('$u(t,x)$')\nax.axis('square')\nax.set_xlim([-1.1,1.1])\nax.set_ylim([-1.1,1.1])    \nax.set_title('$t = 0.75$', fontsize = 10)\n\nsavefig('Burgers.jpg')  ","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:29:14.930185Z","iopub.execute_input":"2021-11-21T04:29:14.930635Z","iopub.status.idle":"2021-11-21T04:29:16.329541Z","shell.execute_reply.started":"2021-11-21T04:29:14.930601Z","shell.execute_reply":"2021-11-21T04:29:16.328853Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Task: Given Data points for a particular PDE: Estimate the correct form of PDE","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PhysicsInformedNN:\n    # Initialize the class\n    def __init__(self, X, u, layers, lb, ub):\n        \n        self.lb = lb\n        self.ub = ub\n        \n        self.x = X[:,0:1]\n        self.t = X[:,1:2]\n        self.u = u\n        \n        self.layers = layers\n        \n        # Initialize NNs\n        self.weights, self.biases = self.initialize_NN(layers)\n        \n        # tf placeholders and graph\n        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                                     log_device_placement=True))\n        \n        # Initialize parameters\n        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n        self.lambda_2 = tf.Variable([-6.0], dtype=tf.float32)\n        \n        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])  # Nonex1\n        self.t_tf = tf.placeholder(tf.float32, shape=[None, self.t.shape[1]])  # Nonex1\n        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])  # Nonex1\n                \n        self.u_pred = self.net_u(self.x_tf, self.t_tf)\n        self.f_pred = self.net_f(self.x_tf, self.t_tf)\n        \n        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n                    tf.reduce_mean(tf.square(self.f_pred))  ## Lambda parameters are incorporated in the loss using f_pred minimization\n        \n        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n                                                                method = 'L-BFGS-B', \n                                                                options = {'maxiter': 50000,\n                                                                           'maxfun': 50000,\n                                                                           'maxcor': 50,\n                                                                           'maxls': 50,\n                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n    \n        self.optimizer_Adam = tf.train.AdamOptimizer()\n        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n        \n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n    def initialize_NN(self, layers):        \n        weights = []\n        biases = []\n        num_layers = len(layers) \n        for l in range(0,num_layers-1):\n            W = self.xavier_init(size=[layers[l], layers[l+1]])\n            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n            weights.append(W)\n            biases.append(b)        \n        return weights, biases\n        \n    def xavier_init(self, size):\n        in_dim = size[0]\n        out_dim = size[1]        \n        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n    \n    def neural_net(self, X, weights, biases):\n        num_layers = len(weights) + 1\n        \n        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n        for l in range(0,num_layers-2):\n            W = weights[l]\n            b = biases[l]\n            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n        W = weights[-1]\n        b = biases[-1]\n        Y = tf.add(tf.matmul(H, W), b)\n        return Y\n            \n    def net_u(self, x, t):  \n        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\n        return u\n    \n    def net_f(self, x, t):\n        lambda_1 = self.lambda_1        \n        lambda_2 = tf.exp(self.lambda_2)\n        u = self.net_u(x,t)\n        u_t = tf.gradients(u, t)[0]\n        u_x = tf.gradients(u, x)[0]\n        u_xx = tf.gradients(u_x, x)[0]\n        f = u_t + lambda_1*u*u_x - lambda_2*u_xx\n        \n        return f\n    \n    def callback(self, loss, lambda_1, lambda_2):\n        print('Loss: %e, l1: %.5f, l2: %.5f' % (loss, lambda_1, np.exp(lambda_2)))\n        \n        \n    def train(self, nIter):\n        tf_dict = {self.x_tf: self.x, self.t_tf: self.t, self.u_tf: self.u}\n        \n        start_time = time.time()\n        for it in range(nIter):\n            self.sess.run(self.train_op_Adam, tf_dict)\n            \n            # Print\n            if it % 10 == 0:\n                elapsed = time.time() - start_time\n                loss_value = self.sess.run(self.loss, tf_dict)\n                lambda_1_value = self.sess.run(self.lambda_1)\n                lambda_2_value = np.exp(self.sess.run(self.lambda_2))\n                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f, Time: %.2f' % \n                      (it, loss_value, lambda_1_value, lambda_2_value, elapsed))\n                start_time = time.time()\n        \n        self.optimizer.minimize(self.sess,\n                                feed_dict = tf_dict,\n                                fetches = [self.loss, self.lambda_1, self.lambda_2],\n                                loss_callback = self.callback)\n        \n        \n    def predict(self, X_star):\n        \n        tf_dict = {self.x_tf: X_star[:,0:1], self.t_tf: X_star[:,1:2]}\n        \n        u_star = self.sess.run(self.u_pred, tf_dict)\n        f_star = self.sess.run(self.f_pred, tf_dict)\n        \n        return u_star, f_star","metadata":{"execution":{"iopub.status.busy":"2021-11-21T07:48:57.505595Z","iopub.execute_input":"2021-11-21T07:48:57.506162Z","iopub.status.idle":"2021-11-21T07:48:57.53256Z","shell.execute_reply.started":"2021-11-21T07:48:57.506124Z","shell.execute_reply":"2021-11-21T07:48:57.53148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nu = 0.01/np.pi\n\nN_u = 2000\nlayers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n\ndata = scipy.io.loadmat('./PINNs/appendix/Data/burgers_shock.mat')\n\nt = data['t'].flatten()[:,None]\nx = data['x'].flatten()[:,None]\nExact = np.real(data['usol']).T  ## Exact solution\n\nX, T = np.meshgrid(x,t)  ## 100x256\n\nX_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) \nu_star = Exact.flatten()[:,None]              \n\n# Doman bounds\nlb = X_star.min(0)\nub = X_star.max(0)    \n\n######################################################################\n######################## Noiseles Data ###############################\n######################################################################\nnoise = 0.0            \n\nidx = np.random.choice(X_star.shape[0], N_u, replace=False)  ## Sample Nu points from entire domain for prediction\nX_u_train = X_star[idx,:]   ## Inputs\nu_train = u_star[idx,:]     ## True Solutions\n\nmodel = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\nmodel.train(0)\n\nu_pred, f_pred = model.predict(X_star)\n\nerror_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n\nU_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n\nlambda_1_value = model.sess.run(model.lambda_1)\nlambda_2_value = model.sess.run(model.lambda_2)\nlambda_2_value = np.exp(lambda_2_value)\n\nerror_lambda_1 = np.abs(lambda_1_value - 1.0)*100\nerror_lambda_2 = np.abs(lambda_2_value - nu)/nu * 100\n\nprint('Error u: %e' % (error_u))    \nprint('Error l1: %.5f%%' % (error_lambda_1))                             \nprint('Error l2: %.5f%%' % (error_lambda_2))  \n\n######################################################################\n########################### Noisy Data ###############################\n######################################################################\nnoise = 0.01        \nu_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])  ## Exact points+ Embedded Noise \n\nmodel = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\nmodel.train(10000)\n\nu_pred, f_pred = model.predict(X_star)\n\nlambda_1_value_noisy = model.sess.run(model.lambda_1)\nlambda_2_value_noisy = model.sess.run(model.lambda_2)\nlambda_2_value_noisy = np.exp(lambda_2_value_noisy)\n\nerror_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0)*100\nerror_lambda_2_noisy = np.abs(lambda_2_value_noisy - nu)/nu * 100\n\nprint('Error lambda_1: %f%%' % (error_lambda_1_noisy))\nprint('Error lambda_2: %f%%' % (error_lambda_2_noisy)) ","metadata":{"execution":{"iopub.status.busy":"2021-11-21T07:49:22.902318Z","iopub.execute_input":"2021-11-21T07:49:22.902629Z","iopub.status.idle":"2021-11-21T07:56:33.868995Z","shell.execute_reply.started":"2021-11-21T07:49:22.902595Z","shell.execute_reply":"2021-11-21T07:56:33.868072Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################################################################\n############################# Plotting ###############################\n######################################################################    \n\nfig, ax = newfig(1.0, 1.4)\nax.axis('off')\n\n####### Row 0: u(t,x) ##################    \ngs0 = gridspec.GridSpec(1, 2)\ngs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\nax = plt.subplot(gs0[:, :])\n\nh = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n              extent=[t.min(), t.max(), x.min(), x.max()], \n              origin='lower', aspect='auto')\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nfig.colorbar(h, cax=cax)\n\nax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 2, clip_on = False)\n\nline = np.linspace(x.min(), x.max(), 2)[:,None]\nax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\nax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\nax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n\nax.set_xlabel('$t$')\nax.set_ylabel('$x$')\nax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\nax.set_title('$u(t,x)$', fontsize = 10)\n\n####### Row 1: u(t,x) slices ##################    \ngs1 = gridspec.GridSpec(1, 3)\ngs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n\nax = plt.subplot(gs1[0, 0])\nax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \nax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\nax.set_xlabel('$x$')\nax.set_ylabel('$u(t,x)$')    \nax.set_title('$t = 0.25$', fontsize = 10)\nax.axis('square')\nax.set_xlim([-1.1,1.1])\nax.set_ylim([-1.1,1.1])\n\nax = plt.subplot(gs1[0, 1])\nax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \nax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\nax.set_xlabel('$x$')\nax.set_ylabel('$u(t,x)$')\nax.axis('square')\nax.set_xlim([-1.1,1.1])\nax.set_ylim([-1.1,1.1])\nax.set_title('$t = 0.50$', fontsize = 10)\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n\nax = plt.subplot(gs1[0, 2])\nax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \nax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\nax.set_xlabel('$x$')\nax.set_ylabel('$u(t,x)$')\nax.axis('square')\nax.set_xlim([-1.1,1.1])\nax.set_ylim([-1.1,1.1])    \nax.set_title('$t = 0.75$', fontsize = 10)\n\n####### Row 2: Identified PDE ##################    \ngs2 = gridspec.GridSpec(1, 3)\ngs2.update(top=1.0-2.0/3.0, bottom=0, left=0.0, right=1.0, wspace=0.0)\n\nax = plt.subplot(gs2[:, :])\nax.axis('off')\n\n#s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.0031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\ns2 = r'Normal Points: $u_t + %.5f u u_x - %.7f u_{xx} = 0$       ' % (lambda_1_value, lambda_2_value)\n#s3 = r'Identified PDE (1\\% noise) & '\ns4 = r'Noisy Points:  $u_t + %.5f u u_x - %.7f u_{xx} = 0$  ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n#s5 = r'\\end{tabular}$'\n#s = s1+s2+s3+s4+s5\ns = s2+s4\nax.text(0.1,0.1,s)\n\nsavefig('Burgers_identification')  ","metadata":{"execution":{"iopub.status.busy":"2021-11-21T08:08:49.290507Z","iopub.execute_input":"2021-11-21T08:08:49.290791Z","iopub.status.idle":"2021-11-21T08:08:50.942287Z","shell.execute_reply.started":"2021-11-21T08:08:49.290761Z","shell.execute_reply":"2021-11-21T08:08:50.941442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}